{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Library imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import math\n",
    "from math import e\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "# Importing model, data loaders and other utilities\n",
    "from models.neo_phurie import NeoPHURIE\n",
    "from dataLoaders.image_loaders import ImageDataset, ImageDataset_Testing, LABELS_STD\n",
    "from utils import EpsilonInsensitiveLoss, OneCycleLR\n",
    "\n",
    "# Constants\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "DATASET_PATH = \"./datasets/images_dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, \n",
    "          num_epochs,\n",
    "          start_learning_rate = 1e-5,\n",
    "          max_learning_rate = 1e-4, \n",
    "          regularization_rate = 0, \n",
    "          train_years = [],\n",
    "          verbose = False):\n",
    "    \n",
    "    # Memory management\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Training Dataset\n",
    "    train_loader = DataLoader(ImageDataset(DATASET_PATH, train_years, transform=transforms.Resize(224), standardization=True), \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True, \n",
    "                              num_workers=4)\n",
    "\n",
    "    # Training optimizer, scheduler and loss\n",
    "    optimizer = optim.SGD(model.parameters(), lr=start_learning_rate, weight_decay=regularization_rate)\n",
    "    scheduler = OneCycleLR(optimizer, num_steps = num_epochs*len(train_loader), lr_range=(start_learning_rate, max_learning_rate))\n",
    "    criterion = EpsilonInsensitiveLoss(0.2, 2)\n",
    "\n",
    "    # Scaler for mixed-precision training\n",
    "    scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    # Moving model to compute device\n",
    "    model.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        # Training Phase\n",
    "        model.train()\n",
    "        train_squared_errors = 0.0\n",
    "\n",
    "        # Progress bar\n",
    "        pbar = tqdm(enumerate(train_loader, 0), total = len(train_loader), unit = \"images\", unit_scale=BATCH_SIZE, disable=not verbose)\n",
    "        \n",
    "        # Iterate through batches in training dataset\n",
    "        for i, data in pbar:\n",
    "            inputs, labels = data\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs.to(device))\n",
    "                loss = criterion(outputs, labels.to(device))\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Display MSE and RMSE on progress bar\n",
    "            train_squared_errors += nn.functional.mse_loss(outputs, labels.to(device)).item() * (LABELS_STD**2) * BATCH_SIZE\n",
    "            pbar.set_description(f\"[Epoch {epoch + 1}]: Training Loss = {train_squared_errors / ((i + 1) * BATCH_SIZE):.4f}: Training RMSE = {math.sqrt(train_squared_errors / ((i + 1) * BATCH_SIZE)):.4f}\")\n",
    "\n",
    "        # Computing final training MSE over entire epoch\n",
    "        train_mse = train_squared_errors / len(train_loader.dataset)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"[Epoch {epoch + 1}]: Training Loss = {train_mse:.4f}; Training RMSE = {math.sqrt(train_mse):.4f}\")\n",
    "\n",
    "        # Memory management\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, testing_years):\n",
    "\n",
    "    def weighted_average_smoothing(predictions):\n",
    "        \"\"\" Method to apply temporal averaging post-processing on model predictions. \"\"\"\n",
    "        out = torch.zeros(predictions.shape)\n",
    "\n",
    "        for i in range(out.shape[0]):\n",
    "            coef = 1 / sum(e ** (np.array(range(0, min(i + 1, 6))) * -0.5))\n",
    "\n",
    "            for j in range(max(i - 5, 0), i+1):\n",
    "                out[i][0] += predictions[j][0] * coef * (e ** (-(i - j) / 2))\n",
    "\n",
    "        return out\n",
    "\n",
    "    # Testing dataset\n",
    "    test_dataset = ImageDataset_Testing(DATASET_PATH, testing_years, transforms.Resize(224), standardization=True)\n",
    "\n",
    "    # Moving model to compute device\n",
    "    model.to(device)\n",
    "\n",
    "    test_squared_errors = torch.tensor(0.0)\n",
    "    errors = []\n",
    "    \n",
    "    for hurricane in test_dataset:\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            inputs, labels = hurricane\n",
    "\n",
    "            with autocast():\n",
    "                outputs = model(inputs.to(device))\n",
    "                smoothed_outputs = weighted_average_smoothing(outputs.cpu())\n",
    "                test_squared_errors += (smoothed_outputs - labels).square().sum()\n",
    "                \n",
    "            errors += ((smoothed_outputs - labels) * LABELS_STD).abs().tolist()\n",
    "\n",
    "            # Memory management\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    test_rmse = (test_squared_errors / test_dataset.get_nr_images()).sqrt()\n",
    "    \n",
    "    return test_rmse.item() * LABELS_STD, np.quantile(errors, 0.25), np.quantile(errors, 0.5), np.quantile(errors, 0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: Training Loss = 241.1904: Training RMSE = 15.5303: 100%|██████████| 102144/102144 [01:49<00:00, 929.63images/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1]: Training Loss = 241.2589; Training RMSE = 15.5325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: Training Loss = 161.2088: Training RMSE = 12.6968: 100%|██████████| 102144/102144 [01:42<00:00, 999.97images/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 2]: Training Loss = 161.2546; Training RMSE = 12.6986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: Training Loss = 120.3663: Training RMSE = 10.9712: 100%|██████████| 102144/102144 [01:45<00:00, 968.22images/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3]: Training Loss = 120.4004; Training RMSE = 10.9727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Epoch 4]: Training Loss = 85.8399: Training RMSE = 9.2650: 100%|██████████| 102144/102144 [01:47<00:00, 953.22images/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4]: Training Loss = 85.8642; Training RMSE = 9.2663\n",
      "Test results: RMSE = 9.41 ; Q1 = 2.11 ; Median = 4.60 ; Q3 = 8.96\n"
     ]
    }
   ],
   "source": [
    "model = NeoPHURIE()\n",
    "\n",
    "train_model(model,\n",
    "      num_epochs = 4,\n",
    "      start_learning_rate = 6e-3,\n",
    "      max_learning_rate = 6e-2,\n",
    "      regularization_rate = 1e-5,\n",
    "      train_years = range(2001, 2015),\n",
    "      verbose = True)\n",
    "\n",
    "rmse, q1, q2, q3 = test_model(model, [2015])\n",
    "print(f\"Test results: RMSE = {rmse:.2f} ; Q1 = {q1:.2f} ; Median = {q2:.2f} ; Q3 = {q3:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing LOYO performance across all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOYO year 2001: RMSE = 8.7412 ; Q1 = 2.1003 ; Median = 4.6629 ; Q3 = 8.8653\n",
      "LOYO year 2002: RMSE = 7.5954 ; Q1 = 1.9732 ; Median = 4.3643 ; Q3 = 8.4156\n",
      "LOYO year 2003: RMSE = 8.0649 ; Q1 = 2.0839 ; Median = 4.5311 ; Q3 = 8.2166\n",
      "LOYO year 2004: RMSE = 8.3169 ; Q1 = 2.1315 ; Median = 4.6315 ; Q3 = 8.2638\n",
      "LOYO year 2005: RMSE = 9.0331 ; Q1 = 2.2656 ; Median = 4.6829 ; Q3 = 8.8538\n",
      "LOYO year 2006: RMSE = 8.2845 ; Q1 = 1.9923 ; Median = 4.2790 ; Q3 = 7.9773\n",
      "LOYO year 2007: RMSE = 8.2793 ; Q1 = 2.1441 ; Median = 4.6410 ; Q3 = 8.5142\n",
      "LOYO year 2008: RMSE = 8.0887 ; Q1 = 1.9185 ; Median = 4.3943 ; Q3 = 8.4330\n",
      "LOYO year 2009: RMSE = 7.8678 ; Q1 = 2.2430 ; Median = 4.6450 ; Q3 = 7.8896\n",
      "LOYO year 2010: RMSE = 8.4903 ; Q1 = 2.1223 ; Median = 4.9156 ; Q3 = 9.0567\n",
      "LOYO year 2011: RMSE = 7.6239 ; Q1 = 2.0982 ; Median = 4.7507 ; Q3 = 8.3725\n",
      "LOYO year 2012: RMSE = 7.4892 ; Q1 = 1.8036 ; Median = 4.0995 ; Q3 = 7.3812\n",
      "LOYO year 2013: RMSE = 7.7556 ; Q1 = 2.0000 ; Median = 4.4084 ; Q3 = 7.9886\n",
      "LOYO year 2014: RMSE = 8.4110 ; Q1 = 2.3424 ; Median = 4.9518 ; Q3 = 8.7308\n",
      "LOYO year 2015: RMSE = 9.0523 ; Q1 = 2.0063 ; Median = 4.5643 ; Q3 = 8.6881\n"
     ]
    }
   ],
   "source": [
    "for test_year in range(2001, 2016):\n",
    "    model = NeoPHURIE()\n",
    "    \n",
    "    train_model(model,\n",
    "      num_epochs = 4,\n",
    "      start_learning_rate = 6e-3,\n",
    "      max_learning_rate = 6e-2,\n",
    "      regularization_rate = 1e-5,\n",
    "      train_years = [y for y in range(2001, 2016) if y != test_year],\n",
    "      verbose = False)\n",
    "    \n",
    "    rmse, q1, q2, q3 = test_model(model, [test_year])\n",
    "    print(f\"LOYO year {test_year}: RMSE = {rmse:.4f} ; Q1 = {q1:.4f} ; Median = {q2:.4f} ; Q3 = {q3:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
